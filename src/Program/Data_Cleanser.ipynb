{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a418b7b",
   "metadata": {},
   "source": [
    "# Loan Approval Data Cleansing\n",
    "\n",
    "This notebook prepares a loan approval dataset for machine learning by cleansing and transforming the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d12bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd5458",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Importing the necessary libraries for data manipulation, preprocessing, and file handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e36749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found at: d:\\School\\CS 434\\Project\\src\\Data\\Dataset.csv\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found at expected path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\School\\CS 434\\Project\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\School\\CS 434\\Project\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\School\\CS 434\\Project\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\School\\CS 434\\Project\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\School\\CS 434\\Project\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# Grab the directory of the current file\n",
    "# Should be ./Project/src/Program\n",
    "try:\n",
    "    current_dir = Path(__file__).parent.absolute()\n",
    "except NameError:\n",
    "    # If using a Jupyter notebook\n",
    "    try:\n",
    "        current_dir = Path.cwd()\n",
    "    except:\n",
    "        # If continuing to fail set path manually\n",
    "        current_dir = Path(\"D:/School/CS 434/Project/src/Program\")\n",
    "\n",
    "# Go to the Data directory \n",
    "# Should be ./Project/src/Data\n",
    "data_dir = current_dir.parent / \"Data\"\n",
    "\n",
    "# Get the Datasets path\n",
    "dataset_path = data_dir / \"Dataset.csv\"\n",
    "\n",
    "# Verify the file exists\n",
    "if dataset_path.exists():\n",
    "    print(f\"Dataset found at: {dataset_path}\")\n",
    "    df = pd.read_csv(dataset_path)\n",
    "else:\n",
    "    print(f\"Dataset not found at expected path: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc2a6a7",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Locating and loading the dataset from the expected path. This ensures we can find our data regardless of the working directory.\n",
    "\n",
    "**Importance**: Proper file path handling is crucial for reproducible data science workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any duplicate rows and remove them\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Convert numeric comlumns to appropriate types\n",
    "numeric_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Categorical columns\n",
    "categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area']\n",
    "\n",
    "# Binary columns\n",
    "binary_columns = ['Gender', 'Married', 'Self_Employed', 'Loan_Status']\n",
    "\n",
    "# Dependents - convert to numeric where possible\n",
    "def clean_dependents(value):\n",
    "    if value == '3+':\n",
    "        return 3\n",
    "    try:\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926be278",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Initial data cleaning steps:\n",
    "- Removing duplicate entries to prevent bias\n",
    "- Converting columns to appropriate data types\n",
    "- Defining categorical and binary columns for proper encoding\n",
    "- Creating a function to handle the special case of '3+' in the Dependents column\n",
    "\n",
    "**Importance**: These steps ensure data integrity and prepare the data for proper encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cabc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleansed dataset saved to 'loan_data_imputed.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kschw\\AppData\\Local\\Temp\\ipykernel_6780\\4293796248.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_imputed[col].fillna(median_value, inplace=True)\n",
      "C:\\Users\\kschw\\AppData\\Local\\Temp\\ipykernel_6780\\4293796248.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_imputed[col].fillna(median_value, inplace=True)\n",
      "C:\\Users\\kschw\\AppData\\Local\\Temp\\ipykernel_6780\\4293796248.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_imputed[col].fillna(median_value, inplace=True)\n",
      "C:\\Users\\kschw\\AppData\\Local\\Temp\\ipykernel_6780\\4293796248.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_imputed[col].fillna(median_value, inplace=True)\n",
      "C:\\Users\\kschw\\AppData\\Local\\Temp\\ipykernel_6780\\4293796248.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_imputed[col].fillna(mode_value, inplace=True)\n",
      "C:\\Users\\kschw\\AppData\\Local\\Temp\\ipykernel_6780\\4293796248.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_imputed['Dependents'].fillna(df_imputed['Dependents'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Create first cleansed dataset - using median/mode imputation\n",
    "df_imputed = df.copy()\n",
    "\n",
    "# Impute missing numeric values with media\n",
    "for col in numeric_columns:\n",
    "    median_value = df_imputed[col].median()\n",
    "    df_imputed[col].fillna(median_value, inplace=True)\n",
    "\n",
    "# Impute missing categorical values with mode\n",
    "for col in categorical_columns:\n",
    "    mode_value = df_imputed[col].mode()[0]\n",
    "    df_imputed[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Binary encoding where applicable\n",
    "for col in binary_columns:\n",
    "    df_imputed[col] = df_imputed[col].map({'Male': 1, 'Female': 0,\n",
    "                                           'Yes': 1, 'No': 0,\n",
    "                                           'Graduate': 1, 'Not Graduate': 0,\n",
    "                                           'Y': 1, 'N': 0}).astype('int')\n",
    "    \n",
    "# For non-binary categorical columns, use one-hot encoding\n",
    "# Propert_Area\n",
    "df_imputed = pd.get_dummies(df_imputed, columns=['Property_Area'], drop_first=True)\n",
    "\n",
    "# Dependents\n",
    "df_imputed['Dependents'] = df_imputed['Dependents'].apply(clean_dependents)\n",
    "df_imputed['Dependents'].fillna(df_imputed['Dependents'].median(), inplace=True)\n",
    "df_imputed['Dependents'] = df_imputed['Dependents'].astype('int')\n",
    "\n",
    "# Credit_History is already binary, just need to convert to int\n",
    "df_imputed['Credit_History'] = df_imputed['Credit_History'].astype('int')\n",
    "\n",
    "# Save the cleansed dataset to CSV file\n",
    "df_imputed.to_csv(os.path.join(data_dir,r'loan_data_imputed.csv'), index=False)\n",
    "print(\"Cleansed dataset saved to 'loan_data_imputed.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0cb7a8",
   "metadata": {},
   "source": [
    "## Imputation Method\n",
    "Creating the first cleansed dataset using imputation:\n",
    "- Missing numeric values filled with median values\n",
    "- Missing categorical values filled with mode (most frequent value)\n",
    "- Binary encoding for binary columns\n",
    "- One-hot encoding for non-binary categorical columns\n",
    "- Special handling for Dependents column\n",
    "\n",
    "**Importance**: Imputation preserves all data points while providing reasonable estimates for missing values, maintaining the original dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af7899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleansed dataset with removed rows saved to 'loan_data_removed.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kschw\\AppData\\Local\\Temp\\ipykernel_6780\\2287071632.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_removed[col] = df_removed[col].map({'Male': 1, 'Female': 0,\n",
      "C:\\Users\\kschw\\AppData\\Local\\Temp\\ipykernel_6780\\2287071632.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_removed[col] = df_removed[col].map({'Male': 1, 'Female': 0,\n",
      "C:\\Users\\kschw\\AppData\\Local\\Temp\\ipykernel_6780\\2287071632.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_removed[col] = df_removed[col].map({'Male': 1, 'Female': 0,\n",
      "C:\\Users\\kschw\\AppData\\Local\\Temp\\ipykernel_6780\\2287071632.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_removed[col] = df_removed[col].map({'Male': 1, 'Female': 0,\n"
     ]
    }
   ],
   "source": [
    "# Create second cleansed dataset - removing rows with missing values\n",
    "df_removed = df.dropna()\n",
    "\n",
    "# Encode categorical variables to binary/numeric\n",
    "# Binary encoding where applicable\n",
    "for col in binary_columns:\n",
    "    df_removed[col] = df_removed[col].map({'Male': 1, 'Female': 0,\n",
    "                                           'Yes': 1, 'No': 0,\n",
    "                                           'Graduate': 1, 'Not Graduate': 0,\n",
    "                                           'Y': 1, 'N': 0}).astype('int')\n",
    "    \n",
    "# For non-binary categorical columns, use one-hot encoding\n",
    "# Propert_Area\n",
    "df_removed = pd.get_dummies(df_removed, columns=['Property_Area'], drop_first=True)\n",
    "\n",
    "# Dependents\n",
    "df_removed['Dependents'] = df_removed['Dependents'].apply(clean_dependents)\n",
    "\n",
    "# Credit History is already binary, just need to convert to int\n",
    "df_removed['Credit_History'] = df_removed['Credit_History'].astype('int')\n",
    "\n",
    "# Save the cleansed dataset to CSV file\n",
    "df_removed.to_csv(os.path.join(data_dir,r'loan_data_removed.csv'), index=False)\n",
    "print(\"Cleansed dataset with removed rows saved to 'loan_data_removed.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b18b93",
   "metadata": {},
   "source": [
    "## Row Removal Method\n",
    "Creating the second cleansed dataset by removing rows with missing values:\n",
    "- Dropping all rows with any missing values\n",
    "- Applying the same encoding transformations as the imputed dataset\n",
    "\n",
    "**Importance**: This approach ensures we only work with complete data points, potentially providing more reliable but fewer training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (367, 13)\n",
      "Imputed dataset shape: (367, 14)\n",
      "Removed dataset shape: (289, 14)\n"
     ]
    }
   ],
   "source": [
    "# Print summary of the cleaning\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Imputed dataset shape: {df_imputed.shape}\")\n",
    "print(f\"Removed dataset shape: {df_removed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27acc15e",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "Comparing the shapes of the original and cleansed datasets:\n",
    "- Original dataset: 367 rows\n",
    "- Imputed dataset: 367 rows (same size, all rows preserved)\n",
    "- Removed dataset: 289 rows (78 rows removed due to missing values)\n",
    "\n",
    "**Importance**: This comparison helps us understand how much data was affected by each cleaning method, which can influence model training decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278f029",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "The cleansed datasets are now ready for:\n",
    "1. Exploratory data analysis\n",
    "2. Feature selection\n",
    "3. Model training and evaluation\n",
    "\n",
    "Both datasets (imputed and row-removed) can be used to compare model performance with different data cleaning approaches.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
